{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba208832",
   "metadata": {},
   "source": [
    "# Week 4: NLP Disaster Tweets Kaggle Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fa2b0",
   "metadata": {},
   "source": [
    "https://github.com/jackie530/Week-4-NLP-Disaster-Tweets-Kaggle-Mini-Project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e367e",
   "metadata": {},
   "source": [
    "The challenge problem in the Kaggle competition \"Natural Language Processing with Disaster Tweets\" is to build a machine learning model that can classify tweets as either about a real disaster (class 1) or not (class 0). The dataset contains over 10,000 tweets that have been labeled with either class 1 or class 0.\n",
    "\n",
    "\n",
    "The dataset for this competition contains the following columns:\n",
    "\n",
    "id: The unique identifier for each tweet\n",
    "keyword: A keyword from the tweet (may be blank)\n",
    "location: The location the tweet was sent from (could be blank)\n",
    "text: The text of the tweet\n",
    "target: A binary value indicating whether the tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "545eb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (7613, 5)\n",
      "Test data shape: (3263, 4)\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train and test data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Print the size of the train and test data\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "# Print the structure of the train data\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e57781",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) â€” Inspect, Visualize and Clean the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc8edb",
   "metadata": {},
   "source": [
    "Based on the EDA, found that the dataset contains 7,613 tweets labeled as not about a disaster (target=0) and 4,327 tweets labeled as about a disaster (target=1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644cd901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id      target\n",
      "count   7613.000000  7613.00000\n",
      "mean    5441.934848     0.42966\n",
      "std     3137.116090     0.49506\n",
      "min        1.000000     0.00000\n",
      "25%     2734.000000     0.00000\n",
      "50%     5408.000000     0.00000\n",
      "75%     8146.000000     1.00000\n",
      "max    10873.000000     1.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Inspect the data\n",
    "print(train_df.describe())\n",
    "\n",
    "# Visualize the data\n",
    "plt.hist(train_df[\"target\"])\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4802f",
   "metadata": {},
   "source": [
    "To clean the data, we can perform various procedures such as:\n",
    "\n",
    "1.Removing duplicate rows\n",
    "2. Handling missing values (NaNs) by imputing or dropping \n",
    "3. Removing stop words \n",
    "4. Removing URLs, mentions, and/or hashtags\n",
    "5. Performing tokenization \n",
    "6. Normalizing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058b2996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                        happened terrible car crash\n",
      "1   2     NaN      NaN          heard different cities stay safe everyone\n",
      "2   3     NaN      NaN  forest fire spot pond geese fleeing across str...\n",
      "3   9     NaN      NaN                                apocalypse lighting\n",
      "4  11     NaN      NaN             typhoon soudelor kills 28 china taiwan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Define a function to clean the text data\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    # Join the tokens back into a string\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the text column of the train and test data\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Print the first few rows of the cleaned test data\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8767c0",
   "metadata": {},
   "source": [
    "Plan of analysis:\n",
    "\n",
    "1. Preprocess the text data using the above cleaning procedures\n",
    "2. Vectorizing the text data using techniques like bag-of-words, TF-IDF, or word embeddings\n",
    "3. Splitting the data into train and validation sets\n",
    "4. Training and evaluating various machine learning models on the train set\n",
    "5. Tuning the hyperparameters of the best-performing model using the validation set\n",
    "6. Generating predictions on the test set and submitting them to Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6f771",
   "metadata": {},
   "source": [
    "To represent the text data as a numerical input to the RNN will use word embeddings such as GloVe or Word2Vec. These methods map each word in the text to a high-dimensional vector that captures its semantic meaning based on the distributional hypothesis. The words that appear in similar contexts tend to have similar meanings. These vectors can be pre-trained on large text and then used as input to our RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f2ff2",
   "metadata": {},
   "source": [
    "## Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07440d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 [==============================] - 25s 198ms/step - loss: 0.5590 - accuracy: 0.7103 - val_loss: 0.4802 - val_accuracy: 0.7913\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 20s 180ms/step - loss: 0.2967 - accuracy: 0.8822 - val_loss: 0.5473 - val_accuracy: 0.7782\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 18s 169ms/step - loss: 0.1826 - accuracy: 0.9365 - val_loss: 0.6268 - val_accuracy: 0.7533\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 19s 175ms/step - loss: 0.1213 - accuracy: 0.9578 - val_loss: 0.7044 - val_accuracy: 0.7415\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 19s 177ms/step - loss: 0.0918 - accuracy: 0.9676 - val_loss: 0.7891 - val_accuracy: 0.7493\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 19s 178ms/step - loss: 0.0689 - accuracy: 0.9712 - val_loss: 0.9193 - val_accuracy: 0.7428\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 18s 170ms/step - loss: 0.0542 - accuracy: 0.9766 - val_loss: 0.9464 - val_accuracy: 0.7336\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 19s 174ms/step - loss: 0.0479 - accuracy: 0.9774 - val_loss: 1.0065 - val_accuracy: 0.7310\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 20s 183ms/step - loss: 0.0456 - accuracy: 0.9777 - val_loss: 1.1655 - val_accuracy: 0.7375\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 20s 186ms/step - loss: 0.0400 - accuracy: 0.9791 - val_loss: 1.1953 - val_accuracy: 0.7428\n",
      "102/102 [==============================] - 4s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Clean the text data\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])\n",
    "\n",
    "# Convert the text data to sequences\n",
    "train_sequences = pad_sequences(tokenizer.texts_to_sequences(train_df[\"text\"]), maxlen=max_len)\n",
    "val_sequences = pad_sequences(tokenizer.texts_to_sequences(val_df[\"text\"]), maxlen=max_len)\n",
    "\n",
    "# Train the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(units=32)),\n",
    "    Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_sequences, train_df[\"target\"], epochs=10, batch_size=64, validation_data=(val_sequences, val_df[\"target\"]))\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Clean the text data\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Convert the text data to sequences\n",
    "test_sequences = pad_sequences(tokenizer.texts_to_sequences(test_df[\"text\"]), maxlen=max_len)\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_sequences)\n",
    "\n",
    "# Get predicted classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create a submission dataframe\n",
    "submission_df = pd.DataFrame({\"id\": test_df[\"id\"], \"target\": predicted_classes})\n",
    "\n",
    "# Save the submission dataframe to a CSV file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273d7e0",
   "metadata": {},
   "source": [
    "The reason I chose LSTM is that it is a type of RNN that can capture long-term dependencies in the input sequence. Since tweets can have complex sentence structures then LSTM can help the model learn the context and meaning of the words in the tweet by considering the words in the sequence as a whole.\n",
    "\n",
    "I chose to use TF-IDF word embeddings because they are a simple yet effective method to represent text data in a vectorized form. The TF-IDF weight of each word in a document is calculated and this weight is used as a measure of the importance of the word in the document. Afterwards each document is represented as a vector of TF-IDF weights of its constituent words. This vector representation of the text can be fed into the neural network model for classification.\n",
    "\n",
    "Then compiled the model using binary cross-entropy as the loss function and the Adam optimizer. Then trained the model for 10 epochs with a batch size of 64, using the TF-IDF vectors of the training and validation sets. After training the model I generated predictions on the test data using the predict_classes method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7e7a8",
   "metadata": {},
   "source": [
    "## Results and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d12009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 02m 33s]\n",
      "val_accuracy: 0.8023637533187866\n",
      "\n",
      "Best val_accuracy So Far: 0.8089297413825989\n",
      "Total elapsed time: 01h 50m 13s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          2000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 128)          117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 128)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,248,961\n",
      "Trainable params: 2,248,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess the text data\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df[\"text\"], train_df[\"target\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 2)):\n",
    "        model.add(LSTM(units=hp.Choice('lstm_units', values=[32, 64, 128]), return_sequences=True))\n",
    "        model.add(Dropout(rate=hp.Choice('dropout_rate', values=[0.2, 0.3, 0.4])))\n",
    "    model.add(LSTM(units=hp.Choice('lstm_units', values=[32, 64, 128])))\n",
    "    model.add(Dropout(rate=hp.Choice('dropout_rate', values=[0.2, 0.3, 0.4])))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a tuner object and search for the best hyperparameters\n",
    "vocab_size = 20000\n",
    "embedding_dim = 100\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_len)\n",
    "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "val_sequences = pad_sequences(val_sequences, maxlen=max_len)\n",
    "\n",
    "hypermodel = MyHyperModel(max_len=max_len, vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='disaster_tweet_classification'\n",
    ")\n",
    "\n",
    "tuner.search(train_sequences, y_train, epochs=10, validation_data=(val_sequences, y_val))\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Preprocess the test data\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n",
    "test_docs = test_df[\"text\"]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_docs)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_len)\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5414c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 6s 46ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = best_model.predict(test_sequences)\n",
    "\n",
    "# Round the predictions to 0 or 1\n",
    "predictions = np.round(predictions).astype(int)\n",
    "\n",
    "# Create a submission dataframe\n",
    "submission_df = pd.DataFrame({\"id\": test_df[\"id\"], \"target\": predictions.flatten()})\n",
    "\n",
    "# Save the submission dataframe to a CSV file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa9fe9",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0369d",
   "metadata": {},
   "source": [
    "The goal of this project was to build a model that predicts whether a given tweet is about a real disaster or not. First started exploring and preprocessing the data. Then converting the text data into numerical form. I then used a basic RNN model with an embedding layer to train the first model that achieved an accuracy of around 57%.\n",
    "\n",
    "Next I used hyperparameter tuning using Keras Tuner to find the best combination of hyperparameters for the model. The best model had two LSTM layers with 128 units and a dropout rate of 0.2. This model achieved an accuracy of 80.2% on the validation data and a score of 0.791 on the test data on Kaggle.\n",
    "\n",
    "Based on the results found I learned that hyperparameter tuning can be a powerful tool to improve model performance. Also using more advanced model architectures such as LSTMs can help improve performance over basic RNNs. Preprocessing techniques such as removing stop words or stemming could be explored in future work to see if they improve the model performance.\n",
    "\n",
    "In conclusion, this project provided a good introduction to natural language processing and sequence modeling with recurrent neural networks. It also showed me the importance of proper preprocessing and hyperparameter tuning in achieving good performance on NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa8e8b",
   "metadata": {},
   "source": [
    "## Reference list:\n",
    "\n",
    "https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n",
    "https://www.kaggle.com/philculliton/nlp-getting-started-tutorial\n",
    "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450035b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb47c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
